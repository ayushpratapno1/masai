{"cells":[{"cell_type":"markdown","id":"dbc79b28","metadata":{"id":"dbc79b28"},"source":["# ðŸš¦ Reinforcement Learning for Traffic Signal Control\n","This notebook demonstrates a simple RL-based simulation to minimize vehicle delays at an intersection."]},{"cell_type":"code","execution_count":null,"id":"89ded5f0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"89ded5f0","executionInfo":{"status":"ok","timestamp":1749212554798,"user_tz":-330,"elapsed":6181,"user":{"displayName":"Tarachand Amgoth","userId":"10918425592946470365"}},"outputId":"f465b7e3-d31b-4263-af5b-261549f1b862"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"]}],"source":["!pip install gym numpy matplotlib"]},{"cell_type":"code","execution_count":null,"id":"b997c430","metadata":{"id":"b997c430"},"outputs":[],"source":["import gym\n","from gym import spaces\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define synthetic traffic environment\n","class TrafficSignalEnv(gym.Env):\n","    def __init__(self):\n","        super(TrafficSignalEnv, self).__init__()\n","        self.max_cars = 20\n","        self.action_space = spaces.Discrete(2)  # 0: keep phase, 1: change phase\n","        self.observation_space = spaces.Box(low=0, high=self.max_cars, shape=(2,), dtype=np.int32)  # [queue_1, queue_2]\n","        self.reset()\n","\n","    def reset(self):\n","        self.queue_1 = np.random.randint(0, self.max_cars)\n","        self.queue_2 = np.random.randint(0, self.max_cars)\n","        self.phase = 0  # 0: green for queue_1, 1: green for queue_2\n","        return np.array([self.queue_1, self.queue_2])\n","\n","    def step(self, action):\n","        if action == 1:\n","            self.phase = 1 - self.phase  # switch phase\n","\n","        if self.phase == 0:\n","            self.queue_1 = max(0, self.queue_1 - np.random.randint(1, 5))\n","            self.queue_2 += np.random.randint(1, 3)\n","        else:\n","            self.queue_2 = max(0, self.queue_2 - np.random.randint(1, 5))\n","            self.queue_1 += np.random.randint(1, 3)\n","\n","        reward = - (self.queue_1 + self.queue_2)  # minimize queue\n","        state = np.array([self.queue_1, self.queue_2])\n","        done = False\n","\n","        return state, reward, done, {}\n","\n","    def render(self, mode='human'):\n","        print(f\"Queue1: {self.queue_1}, Queue2: {self.queue_2}, Phase: {self.phase}\")\n"]},{"cell_type":"code","execution_count":null,"id":"740ff82c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"740ff82c","executionInfo":{"status":"error","timestamp":1749212789984,"user_tz":-330,"elapsed":33,"user":{"displayName":"Tarachand Amgoth","userId":"10918425592946470365"}},"outputId":"72058a6e-de46-4947-f7fb-e179b70bec19"},"outputs":[{"output_type":"error","ename":"IndexError","evalue":"invalid index to scalar variable.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-b5fae05e9e85>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mphase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."]}],"source":["env = TrafficSignalEnv()\n","q_table = np.zeros((21, 21, 2))  # (queue1, queue2, phase) â†’ 2 actions\n","alpha = 0.1\n","gamma = 0.95\n","epsilon = 1.0\n","episodes = 1000\n","reward_log = []\n","\n","for ep in range(episodes):\n","    state = env.reset()\n","    phase = env.phase\n","    total_reward = 0\n","    for _ in range(50):  # max steps\n","        s = (state[0], state[1], phase)\n","        if np.random.rand() < epsilon:\n","            action = env.action_space.sample()\n","        else:\n","            action = np.argmax(q_table[s])\n","\n","        next_state, reward, _, _ = env.step(action)\n","        next_phase = env.phase\n","        ns = (next_state[0], next_state[1], next_phase)\n","\n","\n","        q_table[s][action] += alpha * (reward + gamma * np.max(q_table[ns]) - q_table[s][action])\n","        state = next_state\n","        phase = next_phase\n","        total_reward += reward\n","\n","    epsilon = max(0.01, epsilon * 0.995)\n","    reward_log.append(total_reward)\n","\n","plt.plot(reward_log)\n","plt.title(\"Training Reward over Episodes\")\n","plt.xlabel(\"Episode\")\n","plt.ylabel(\"Total Reward\")\n","plt.show()"]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}